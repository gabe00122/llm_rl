import time
from typing import Any, Literal, NamedTuple
import jax
from jax import numpy as jnp
from llmrl.util import batched_put
import numpy as np
from flax import nnx

# from tokenizers import Tokenizer
from distrax import Categorical
from transformers import PreTrainedTokenizerFast

from rich.console import Console
from rich.markdown import Markdown

from llmrl.config import LoraConfig, SamplingConfig
from llmrl.model import Qwen3
from llmrl.checkpoint import load_model

PAD_ID = 151643
EOS_1 = 151645
EOS_2 = 151643
# END_TOKEN = 151644


class GenerationState(NamedTuple):
    kv_cache: Any

    # where the model is in the context in term of evaluation
    kv_cache_length: jax.Array
    # how long the context is
    context_length: jax.Array
    # this indicates where the prompt turn ends and the conversation turn starts
    turn_start_positions: jax.Array
    # true for prompt tokens and false for tokens generated by the model
    prompt_mask: jax.Array
    # tokens for the context, this includes both prompts and generated tokens
    context: jax.Array

    # gen variables
    # next_tokens: jax.Array
    turn_finished: jax.Array
    rng_key: jax.Array

    # rollout
    log_probs: jax.Array
    values: jax.Array


def encode_input(
    tokenizer: PreTrainedTokenizerFast,
    conversations: list[list[dict]] | list[dict],
):
    return tokenizer.apply_chat_template(
        conversations,
        add_generation_prompt=True,
        return_tensors="np",
        enable_thinking=False
    )


def decode_responses(
    tokenizer: PreTrainedTokenizerFast,
    gen: 'GenerationState',
) -> list[str]:
    (indices,) = jnp.where(gen.turn_finished)

    output = []
    for b in indices.tolist():
        output.append(tokenizer.decode(np.asarray(gen.context[b, gen.turn_start_positions[b] : gen.context_length[b]-2])))

    return output


def sample(config: SamplingConfig, logits, rng_key):
    V = logits.shape[-1]
    k = min(config.top_k, V)

    topk_logits, topk_idx = jax.lax.top_k(logits, k)  # (..., k)
    topk_logits = topk_logits / config.temperature

    topk_probs = jax.nn.softmax(topk_logits, axis=-1)  # (..., k)

    cumsum = jnp.cumsum(topk_probs, axis=-1) - topk_probs
    masked_topk_logits = jnp.where(cumsum < config.top_p, topk_logits, -jnp.inf)

    sample_in_topk = jax.random.categorical(rng_key, masked_topk_logits, axis=-1)
    sample_in_topk = jnp.expand_dims(sample_in_topk, axis=-1)
    sampled_ids = jnp.take_along_axis(topk_idx, sample_in_topk, axis=-1).squeeze(-1)

    return sampled_ids


def create_generation_state(
    kv_cache, batch_size: int, seq_len: int, rng_key: jax.Array
) -> GenerationState:
    return GenerationState(
        kv_cache=kv_cache,
        kv_cache_length=jnp.zeros(batch_size, jnp.int32),
        context_length=jnp.zeros(batch_size, jnp.int32),
        turn_start_positions=jnp.zeros(batch_size, jnp.int32),
        prompt_mask=jnp.zeros((batch_size, seq_len), jnp.bool_),
        context=jnp.zeros((batch_size, seq_len), jnp.int32),
        # next_tokens=jnp.zeros(batch_size, jnp.int32),
        turn_finished=jnp.zeros(batch_size, jnp.bool_),
        log_probs=jnp.zeros((batch_size, seq_len), jnp.float32),
        values=jnp.zeros((batch_size, seq_len), jnp.float32),
        rng_key=rng_key,
    )


def reset_generation_state(state: GenerationState) -> GenerationState:
    return state._replace(
        kv_cache_length=jnp.zeros_like(state.kv_cache_length),
        context_length=jnp.zeros_like(state.context_length),
        turn_start_positions=jnp.zeros_like(state.turn_start_positions),
        prompt_mask=jnp.zeros_like(state.prompt_mask),
        turn_finished=jnp.zeros_like(state.turn_finished),
    )


def append_prompt_tokens(
    state: GenerationState, batch_indices: jax.Array, prompts: list[np.ndarray]
) -> GenerationState:
    context = state.context
    prompt_mask = state.prompt_mask
    turn_start_positions = state.turn_start_positions
    turn_finished = state.turn_finished.at[batch_indices].set(False)
    # todo: could be more efficient

    for i, prompt in zip(batch_indices.tolist(), prompts):
        start = state.context_length[i]
        end = start + prompt.shape[0]
        context = context.at[i, start:end].set(prompt)
        prompt_mask = prompt_mask.at[i, start:end].set(np.ones_like(prompt, np.bool_))
        
        turn_start_positions = turn_start_positions.at[i].set(end)

    return state._replace(
        context=context,
        prompt_mask=prompt_mask,
        turn_start_positions=turn_start_positions,
        context_length=jnp.copy(turn_start_positions),
        turn_finished=turn_finished
    )


@jax.jit(static_argnames=("model_def", "sampling"), donate_argnames=("gen",))
def generate(
    model_def,
    model_state,
    sampling: SamplingConfig | Literal["greedy", "simple"],
    gen: GenerationState,
) -> GenerationState:
    model = nnx.merge(model_def, model_state)

    B, seq_length = gen.context.shape
    batch_index = jnp.arange(B, dtype=jnp.int32)

    def cond(carry: GenerationState):
        # todo: let's check for n number of finished episodes
        return jnp.logical_not(jnp.any(carry.turn_finished))

    def body(carry: GenerationState):
        in_tokens = carry.context[batch_index, carry.kv_cache_length]

        logits, value, kv_cache = model(
            in_tokens[..., None],
            carry.kv_cache_length[..., None],
            carry.kv_cache,
        )

        sample_key, rng_key = jax.random.split(carry.rng_key)

        next_log_probs = gen.log_probs

        next_values = gen.values.at[batch_index, carry.kv_cache_length].set(value.squeeze(-1))
        if sampling == "greedy":
            sample_tokens = jnp.argmax(logits.squeeze(-2), -1)
        elif sampling == "simple":
            dist = Categorical(logits=logits.squeeze(-2))
            sample_tokens: jax.Array = dist.sample(seed=sample_key)
            log_prob = dist.log_prob(sample_tokens)
            next_log_probs = next_log_probs.at[batch_index, carry.kv_cache_length].set(log_prob)
        else:
            sample_tokens = sample(sampling, logits.squeeze(axis=-2), sample_key)

        next_kv_cache_length = carry.kv_cache_length + 1
        next_context_length = jnp.maximum(carry.context_length, next_kv_cache_length + 1)

        use_sample = next_kv_cache_length >= carry.turn_start_positions
        out_tokens = jnp.where(
            use_sample, sample_tokens, carry.context[batch_index, next_kv_cache_length]
        )

        next_context = carry.context.at[batch_index, next_kv_cache_length].set(out_tokens)

        # check for finished states
        next_finished = jnp.logical_or(
            carry.turn_finished, next_context_length >= seq_length
        )
        # we check to make sure we aren't reading the prompt and that the model sampled a end token
        next_finished = jnp.logical_or(
            next_finished, jnp.logical_and(in_tokens == EOS_1, use_sample)
        )

        # jax.debug.print("{} - {}", use_sample[0], out_tokens[0])

        return carry._replace(
            kv_cache=kv_cache,
            kv_cache_length=next_kv_cache_length,
            context_length=next_context_length,
            context=next_context,
            turn_finished=next_finished,
            log_probs=next_log_probs,
            values=next_values,
            rng_key=rng_key,
        )

    return nnx.while_loop(cond, body, gen)


def chat(
    console: Console,
    model: Qwen3,
    tokenizer,
    sampling,
    batch_size: int,
    seq_length: int,
    rngs: nnx.Rngs,
):
    model_def, model_state = nnx.split(model)

    rng_key = rngs.sample()
    kv_cache = model.initialize_carry(batch_size, seq_length)

    gen = create_generation_state(kv_cache, batch_size, seq_length, rng_key)

    while True:
        prompt = console.input("Prompt:\n")

        if prompt == "/clear":
            gen = reset_generation_state(gen)
            continue
        
        text = [{"role": "user", "content": prompt} for _ in range(batch_size)]
        prompt_tokens = encode_input(tokenizer, text)

        gen = append_prompt_tokens(gen, jnp.arange(batch_size), prompt_tokens)

        # start_time = time.time()

        gen: GenerationState = generate(model_def, model_state, "simple", gen)

        console.print(gen.log_probs)
        console.print(gen.values)

        output_text: list[str] = decode_responses(
            tokenizer, gen
        )
        for out in output_text:
            assistant = out  # get_last_turn(out)
            console.print("--------")
            console.print(assistant)

        # stop_time = time.time()
        # delta_time = stop_time - start_time
        # total_tokens = jnp.sum(gen.positions - start_pos).item()
        # console.print(f"TPS: {total_tokens // delta_time}")
        # console.print(f"Context: {gen.positions[0].item()}/{seq_length}")


def main():
    # model_path = "./base-models/qwen3-0.6b"
    model_path = "./base-models/Qwen3-4B-Instruct-2507"
    # model_path = "./base-models/Qwen3-4B-Thinking-2507"
    lora_config = LoraConfig(False, False, 0)
    rngs = nnx.Rngs(0)
    model, tokenizer, sampling = load_model(model_path, lora_config, rngs)

    batch_size = 1
    seq_length =  16384  # 512
    chat(Console(), model, tokenizer, sampling, batch_size, seq_length, rngs)


if __name__ == "__main__":
    main()
