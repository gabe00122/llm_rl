import time
from typing import Any, Literal, NamedTuple

import jax
import numpy as np

# from tokenizers import Tokenizer
from distrax import Categorical
from flax import nnx
from jax import numpy as jnp
from llmrl.base_model_loader import load_base_model
from llmrl.checkpointer import Checkpointer
from llmrl.config import SamplingConfig
from llmrl.experiement import Experiment
from llmrl.model import Qwen3
from llmrl.model.value_network import ValueParam
from llmrl.util import batched_put, batched_put_where, batched_take
from rich.console import Console
from rich.markdown import Markdown
from transformers import PreTrainedTokenizerFast

PAD_ID = 151643
EOS_1 = 151645
EOS_2 = 151643
# END_TOKEN = 151644


class GenerationState(NamedTuple):
    kv_cache: Any

    # where the model is in the context in term of evaluation
    kv_cache_length: jax.Array
    # how long the context is
    context_length: jax.Array
    # this indicates where the prompt turn ends and the conversation turn starts
    turn_start_positions: jax.Array
    # the reset position for the environment so it dosn't need to reprocess the start instructions
    env_instruction_length: jax.Array
    # true for prompt tokens and false for tokens generated by the model
    policy_mask: jax.Array
    # tokens for the context, this includes both prompts and generated tokens
    context: jax.Array

    # gen variables
    # next_tokens: jax.Array
    turn_finished: jax.Array
    rng_key: jax.Array

    # rollout
    log_probs: jax.Array
    values: jax.Array


def encode_input(
    tokenizer: PreTrainedTokenizerFast,
    conversations: list[list[dict]],
    add_generation_prompt=True,
) -> list[np.ndarray]:
    return tokenizer.apply_chat_template(
        conversations,
        add_generation_prompt=add_generation_prompt,
        return_tensors="np",
    )


def decode_responses(
    tokenizer: PreTrainedTokenizerFast,
    gen: GenerationState,
) -> tuple[np.ndarray, list[str]]:
    (indices,) = np.where(gen.turn_finished)
    indices = indices.astype(np.int32)

    context = np.array(gen.context)
    turn_start_positions = np.array(gen.turn_start_positions)
    context_length = np.array(gen.context_length)

    output = [
        tokenizer.decode(context[b, turn_start_positions[b] : context_length[b] - 1])
        for b in indices
    ]

    return indices, output


def sample(config: SamplingConfig, logits, rng_key):
    V = logits.shape[-1]
    k = min(config.top_k, V)

    topk_logits, topk_idx = jax.lax.top_k(logits, k)  # (..., k)
    topk_logits = topk_logits / config.temperature

    topk_probs = jax.nn.softmax(topk_logits, axis=-1)  # (..., k)

    cumsum = jnp.cumsum(topk_probs, axis=-1) - topk_probs
    masked_topk_logits = jnp.where(cumsum < config.top_p, topk_logits, -jnp.inf)

    sample_in_topk = jax.random.categorical(rng_key, masked_topk_logits, axis=-1)
    sample_in_topk = jnp.expand_dims(sample_in_topk, axis=-1)
    sampled_ids = jnp.take_along_axis(topk_idx, sample_in_topk, axis=-1).squeeze(-1)

    return sampled_ids


def create_generation_state(
    kv_cache, batch_size: int, seq_len: int, rng_key: jax.Array
) -> GenerationState:
    return GenerationState(
        kv_cache=kv_cache,
        kv_cache_length=jnp.zeros(batch_size, jnp.int32),
        context_length=jnp.zeros(batch_size, jnp.int32),
        turn_start_positions=jnp.zeros(batch_size, jnp.int32),
        env_instruction_length=jnp.zeros(batch_size, jnp.int32),
        policy_mask=jnp.zeros((batch_size, seq_len), jnp.bool_),
        context=jnp.zeros((batch_size, seq_len), jnp.int32),
        turn_finished=jnp.zeros(batch_size, jnp.bool_),
        log_probs=jnp.zeros((batch_size, seq_len - 1), jnp.float32),
        values=jnp.zeros((batch_size, seq_len), jnp.float32),
        rng_key=rng_key,
    )


def reset_generation_state(state: GenerationState) -> GenerationState:
    return state._replace(
        kv_cache_length=jnp.zeros_like(state.kv_cache_length),
        context_length=state.env_instruction_length.copy(),
    )


def append_prompt_tokens(
    state: GenerationState, batch_indices: np.ndarray, prompts: list[np.ndarray]
) -> GenerationState:
    context = np.array(state.context)
    # prompt_mask = state.prompt_mask
    context_length = np.array(state.context_length)
    turn_start_positions = np.array(state.turn_start_positions)
    turn_finished = np.array(state.turn_finished)
    turn_finished[batch_indices] = False
    # todo: could be more efficient

    for i, prompt in zip(batch_indices, prompts):
        start = context_length[i].item()
        end = start + prompt.shape[0]
        safe_end = min(end, context.shape[1])
        if safe_end < end:
            delta = end - safe_end
            prompt = prompt[:-delta]

        context[i, start:safe_end] = prompt

        turn_start_positions[i] = safe_end

    return state._replace(
        context=context,
        # prompt_mask=prompt_mask,
        turn_start_positions=jnp.array(turn_start_positions),
        context_length=jnp.array(turn_start_positions),
        turn_finished=jnp.array(turn_finished),
    )


def append_user_prompts(
    state: GenerationState,
    batch_indices: np.ndarray,
    tokenizer: PreTrainedTokenizerFast,
    prompts: list[str],
):
    conversation_turns = [[{"role": "user", "content": content}] for content in prompts]
    prompt_tokens = encode_input(tokenizer, conversation_turns)

    return append_prompt_tokens(state, batch_indices, prompt_tokens)


@jax.jit(donate_argnums=(0,))
def reset_episodes(state: GenerationState, done_mask: jax.Array) -> GenerationState:
    return state._replace(
        context_length=jnp.where(
            done_mask, state.env_instruction_length, state.context_length
        ),
        # set to zero to stay more on policy
        kv_cache_length=jnp.where(
            done_mask, 0, state.kv_cache_length
        ),  # jnp.where(done_mask, state.env_instruction_length, state.kv_cache_length),
    )


@jax.jit(
    static_argnames=("model_def", "sampling", "wait_for"), donate_argnames=("gen",)
)
def generate(
    model_def,
    model_state,
    sampling: SamplingConfig | Literal["greedy", "simple"],
    gen: GenerationState,
    wait_for: int = 1,
) -> GenerationState:
    model = nnx.merge(model_def, model_state)

    B, seq_length = gen.context.shape

    def cond(carry: GenerationState):
        # todo: this could infinite loop
        return jnp.sum(carry.turn_finished) < wait_for

    def body(carry: GenerationState):
        in_tokens = batched_take(carry.context, carry.kv_cache_length)

        logits, value, kv_cache = model(
            in_tokens[..., None],  # add time axis
            carry.kv_cache_length[..., None],
            carry.kv_cache,
        )
        logits = logits.squeeze(-2)  # remove time axis
        value = value.squeeze(-1)

        sample_key, rng_key = jax.random.split(carry.rng_key)

        dist = Categorical(logits=logits)
        sample_tokens: jax.Array = dist.sample(seed=sample_key)
        log_prob: jax.Array = dist.log_prob(sample_tokens)

        # store everything
        over_start_position = carry.kv_cache_length + 1 >= carry.turn_start_positions
        turn_finished = carry.turn_finished | (carry.kv_cache_length + 2 >= seq_length)
        use_sample = ~turn_finished & over_start_position

        kv_cache_length = jnp.where(
            turn_finished, carry.kv_cache_length, carry.kv_cache_length + 1
        )
        context_length = jnp.where(
            turn_finished,
            carry.context_length,
            jnp.maximum(carry.context_length, carry.kv_cache_length + 2),
        )

        context = batched_put_where(
            carry.context, kv_cache_length, sample_tokens, use_sample
        )
        log_probs = batched_put_where(
            carry.log_probs, carry.kv_cache_length, log_prob, use_sample
        )
        # we want to track values even for prompt tokens, hence no use_sample check
        values = batched_put_where(
            carry.values, carry.kv_cache_length, value, ~turn_finished
        )

        # we might be able to drop the ~turn_finished filter here
        policy_mask = batched_put_where(
            carry.policy_mask, carry.kv_cache_length, use_sample, ~turn_finished
        )

        turn_finished = turn_finished | ((in_tokens == EOS_1) & over_start_position)

        return carry._replace(
            kv_cache=kv_cache,
            kv_cache_length=kv_cache_length,
            context_length=context_length,
            context=context,
            turn_finished=turn_finished,
            log_probs=log_probs,
            values=values,
            policy_mask=policy_mask,
            rng_key=rng_key,
        )

    return nnx.while_loop(cond, body, gen)


def chat(
    console: Console,
    model: Qwen3,
    tokenizer,
    sampling,
    batch_size: int,
    seq_length: int,
    rngs: nnx.Rngs,
    *,
    system_prompt: str | None = None,
):
    model_def, model_state = nnx.split(model)

    rng_key = rngs.sample()
    kv_cache = model.initialize_carry(batch_size, seq_length)

    gen = create_generation_state(kv_cache, batch_size, seq_length, rng_key)

    batch_indices = np.arange(batch_size, dtype=np.int32)

    if system_prompt is not None:
        text = [
            [{"role": "system", "content": system_prompt}] for _ in range(batch_size)
        ]
        prompt_tokens = encode_input(tokenizer, text, add_generation_prompt=False)
        gen = append_prompt_tokens(gen, batch_indices, prompt_tokens)

    while True:
        prompt = console.input("Prompt: ")

        if prompt == "/clear":
            gen = reset_generation_state(gen)
            continue

        text = [[{"role": "user", "content": prompt}] for _ in range(batch_size)]
        prompt_tokens = encode_input(tokenizer, text)

        start_time = time.time()
        start_tokens = gen.kv_cache_length[0].item()
        gen = append_prompt_tokens(gen, batch_indices, prompt_tokens)
        gen: GenerationState = generate(model_def, model_state, "simple", gen)
        end_tokens = gen.kv_cache_length[0].item()
        end_time = time.time()

        _, output_text = decode_responses(tokenizer, gen)
        console.print("--------")
        console.print(Markdown(output_text[0]))
        console.print(
            f"TPS: {(end_tokens - start_tokens) / (end_time - start_time):.2}"
        )


def main():
    experiment = Experiment.load("winged-tortoise-of-glory")
    config = experiment.config

    rngs = nnx.Rngs(experiment.params_seed)
    model, tokenizer, sampling = load_base_model(config.base_model, rngs)
    model.initialize_lora(config.lora, rngs=rngs)

    checkpointer = Checkpointer(experiment.checkpoints_url)
    checkpointer.restore_latest(
        {"opt": orbax.checkpoint.PLACEHOLDER, "model": model},
        nnx.Any(ValueParam, nnx.LoRAParam),
    )

    batch_size = 1
    seq_length = 16384  # 512
    chat(
        Console(),
        model,
        tokenizer,
        sampling,
        batch_size,
        seq_length,
        rngs,
        system_prompt="Solve the arithmetic expression using +, -, * or /. Show your work if needed, but end with only the numeric result on its own line. Always output with decimals such as 123.456",
    )


if __name__ == "__main__":
    main()
