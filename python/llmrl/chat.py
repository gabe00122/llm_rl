import time
from typing import Any, Literal, NamedTuple
import jax
from jax import numpy as jnp
from llmrl.util import batched_put, batched_take
import numpy as np
from flax import nnx

# from tokenizers import Tokenizer
from distrax import Categorical
from transformers import PreTrainedTokenizerFast

from rich.console import Console
from rich.markdown import Markdown

from llmrl.config import LoraConfig, SamplingConfig
from llmrl.model import Qwen3
from llmrl.base_model_loader import load_model

PAD_ID = 151643
EOS_1 = 151645
EOS_2 = 151643
# END_TOKEN = 151644


class GenerationState(NamedTuple):
    kv_cache: Any

    # where the model is in the context in term of evaluation
    kv_cache_length: jax.Array
    # how long the context is
    context_length: jax.Array
    # this indicates where the prompt turn ends and the conversation turn starts
    turn_start_positions: jax.Array
    # the reset position for the environment so it dosn't need to reprocess the start instructions
    env_instruction_length: jax.Array
    # true for prompt tokens and false for tokens generated by the model
    policy_mask: jax.Array
    # tokens for the context, this includes both prompts and generated tokens
    context: jax.Array

    # gen variables
    # next_tokens: jax.Array
    turn_finished: jax.Array
    rng_key: jax.Array

    # rollout
    log_probs: jax.Array
    values: jax.Array


def encode_input(
    tokenizer: PreTrainedTokenizerFast,
    conversations: list[list[dict]],
    add_generation_prompt=True,
) -> list[np.ndarray]:
    return tokenizer.apply_chat_template(
        conversations,
        add_generation_prompt=add_generation_prompt,
        return_tensors="np",
    )


def decode_responses(
    tokenizer: PreTrainedTokenizerFast,
    gen: GenerationState,
) -> tuple[np.ndarray, list[str]]:
    (indices,) = np.where(gen.turn_finished)
    indices = indices.astype(np.int32)

    context = np.array(gen.context)
    turn_start_positions = np.array(gen.turn_start_positions)
    context_length = np.array(gen.context_length)

    output = [tokenizer.decode(context[b, turn_start_positions[b] : context_length[b] - 2]) for b in indices]

    return indices, output


def sample(config: SamplingConfig, logits, rng_key):
    V = logits.shape[-1]
    k = min(config.top_k, V)

    topk_logits, topk_idx = jax.lax.top_k(logits, k)  # (..., k)
    topk_logits = topk_logits / config.temperature

    topk_probs = jax.nn.softmax(topk_logits, axis=-1)  # (..., k)

    cumsum = jnp.cumsum(topk_probs, axis=-1) - topk_probs
    masked_topk_logits = jnp.where(cumsum < config.top_p, topk_logits, -jnp.inf)

    sample_in_topk = jax.random.categorical(rng_key, masked_topk_logits, axis=-1)
    sample_in_topk = jnp.expand_dims(sample_in_topk, axis=-1)
    sampled_ids = jnp.take_along_axis(topk_idx, sample_in_topk, axis=-1).squeeze(-1)

    return sampled_ids


def create_generation_state(
    kv_cache, batch_size: int, seq_len: int, rng_key: jax.Array
) -> GenerationState:
    return GenerationState(
        kv_cache=kv_cache,
        kv_cache_length=jnp.zeros(batch_size, jnp.int32),
        context_length=jnp.zeros(batch_size, jnp.int32),
        turn_start_positions=jnp.zeros(batch_size, jnp.int32),
        env_instruction_length=jnp.zeros(batch_size, jnp.int32),
        policy_mask=jnp.zeros((batch_size, seq_len), jnp.bool_),
        context=jnp.zeros((batch_size, seq_len), jnp.int32),
        turn_finished=jnp.zeros(batch_size, jnp.bool_),
        log_probs=jnp.zeros((batch_size, seq_len), jnp.float32),
        values=jnp.zeros((batch_size, seq_len), jnp.float32),
        rng_key=rng_key,
    )


def reset_generation_state(state: GenerationState) -> GenerationState:
    return state._replace(
        kv_cache_length=jnp.zeros_like(state.kv_cache_length),
        context_length=jnp.zeros_like(state.context_length),
        turn_start_positions=jnp.zeros_like(state.turn_start_positions),
        policy_mask=jnp.zeros_like(state.policy_mask),
        turn_finished=jnp.zeros_like(state.turn_finished),
    )


def append_prompt_tokens(
    state: GenerationState, batch_indices: np.ndarray, prompts: list[np.ndarray]
) -> GenerationState:
    context = np.array(state.context)
    # prompt_mask = state.prompt_mask
    context_length = np.array(state.context_length)
    turn_start_positions = np.array(state.turn_start_positions)
    turn_finished = np.array(state.turn_finished)
    turn_finished[batch_indices] = False
    # todo: could be more efficient

    for i, prompt in zip(batch_indices, prompts):
        start = context_length[i].item()
        end = start + prompt.shape[0]
        context[i, start:end] = prompt
        # prompt_mask = prompt_mask.at[i, start:end].set(np.ones_like(prompt, np.bool_))

        turn_start_positions[i] = end

    return state._replace(
        context=context,
        # prompt_mask=prompt_mask,
        turn_start_positions=jnp.array(turn_start_positions),
        context_length=jnp.array(turn_start_positions),
        turn_finished=jnp.array(turn_finished),
    )

def append_user_prompts(state: GenerationState, batch_indices: np.ndarray, tokenizer: PreTrainedTokenizerFast, prompts: list[str]):
    conversation_turns = [[{"role": "user", "content": content}] for content in prompts]
    prompt_tokens = encode_input(tokenizer, conversation_turns)

    return append_prompt_tokens(state, batch_indices, prompt_tokens)

@jax.jit(donate_argnums=(0,))
def reset_episodes(state: GenerationState, done_mask: jax.Array) -> GenerationState:
    return state._replace(
        context_length = jnp.where(done_mask, state.env_instruction_length, state.context_length),
        # set to zero to stay more on policy
        kv_cache_length = jnp.where(done_mask, 0, state.kv_cache_length), #jnp.where(done_mask, state.env_instruction_length, state.kv_cache_length),
    )

@jax.jit(static_argnames=("model_def", "sampling", "wait_for"), donate_argnames=("gen",))
def generate(
    model_def,
    model_state,
    sampling: SamplingConfig | Literal["greedy", "simple"],
    gen: GenerationState,
    wait_for: int = 1,
) -> GenerationState:
    model = nnx.merge(model_def, model_state)

    B, seq_length = gen.context.shape
    # batch_range = jnp.arange(B, dtype=jnp.int32)

    def cond(carry: GenerationState):
        # todo: this could infinite loop
        return jnp.sum(carry.turn_finished) < wait_for

    def body(carry: GenerationState):
        # in_tokens = carry.context[batch_index, carry.kv_cache_length]
        in_tokens = batched_take(carry.context, carry.kv_cache_length)

        logits, value, kv_cache = model(
            in_tokens[..., None],
            carry.kv_cache_length[..., None],
            carry.kv_cache,
        )

        sample_key, rng_key = jax.random.split(carry.rng_key)

        next_log_probs = gen.log_probs

        # next_values = gen.values.at[batch_index, carry.kv_cache_length].set(
        #     value.squeeze(-1)
        # )
        next_values = batched_put(gen.values, carry.kv_cache_length, value.squeeze(-1))
        if sampling == "greedy":
            sample_tokens = jnp.argmax(logits.squeeze(-2), -1)
        elif sampling == "simple":
            dist = Categorical(logits=logits.squeeze(-2))
            sample_tokens: jax.Array = dist.sample(seed=sample_key)
            log_prob: jax.Array = dist.log_prob(sample_tokens)
            # prob = dist.prob(sample_tokens)
            # next_log_probs = next_log_probs.at[batch_range, carry.kv_cache_length].set(log_prob)
            next_log_probs = batched_put(next_log_probs, carry.kv_cache_length, log_prob)

        else:
            sample_tokens = sample(sampling, logits.squeeze(axis=-2), sample_key)

        next_kv_cache_length = jnp.where(carry.turn_finished, carry.kv_cache_length, carry.kv_cache_length + 1)
        next_context_length = jnp.maximum(
            carry.context_length, next_kv_cache_length + 1
        )

        use_sample = next_kv_cache_length >= carry.turn_start_positions
        out_tokens = jnp.where(
            use_sample, sample_tokens, batched_take(carry.context, next_kv_cache_length)
        )

        next_context = batched_put(carry.context, next_kv_cache_length, out_tokens)

        # check for finished states
        next_finished = jnp.logical_or(
            carry.turn_finished, next_context_length >= seq_length
        )
        # we check to make sure we aren't reading the prompt and that the model sampled a end token
        next_finished = jnp.logical_or(
            next_finished, jnp.logical_and(in_tokens == EOS_1, use_sample)
        )

        policy_mask = batched_put(carry.policy_mask, carry.kv_cache_length, use_sample)

        # jax.debug.print("{} - {}", use_sample[0], out_tokens[0])

        return carry._replace(
            kv_cache=kv_cache,
            kv_cache_length=next_kv_cache_length,
            context_length=next_context_length,
            context=next_context,
            turn_finished=next_finished,
            log_probs=next_log_probs,
            values=next_values,
            policy_mask=policy_mask,
            rng_key=rng_key,
        )

    return nnx.while_loop(cond, body, gen)


def chat(
    console: Console,
    model: Qwen3,
    tokenizer,
    sampling,
    batch_size: int,
    seq_length: int,
    rngs: nnx.Rngs,
):
    model_def, model_state = nnx.split(model)

    rng_key = rngs.sample()
    kv_cache = model.initialize_carry(batch_size, seq_length)

    gen = create_generation_state(kv_cache, batch_size, seq_length, rng_key)

    batch_indecies = np.arange(batch_size, dtype=np.int32)

    while True:
        prompt = console.input("Prompt: ")

        if prompt == "/clear":
            gen = reset_generation_state(gen)
            continue

        text = [[{"role": "user", "content": prompt}] for _ in range(batch_size)]
        prompt_tokens = encode_input(tokenizer, text)

        start_time = time.time()
        start_tokens = gen.kv_cache_length[0].item()
        gen = append_prompt_tokens(gen, batch_indecies, prompt_tokens)
        gen: GenerationState = generate(model_def, model_state, "simple", gen)
        end_tokens = gen.kv_cache_length[0].item()
        end_time = time.time()

        _, output_text = decode_responses(tokenizer, gen)
        console.print("--------")
        console.print(Markdown(output_text[0]))
        console.print(f"TPS: {(end_tokens - start_tokens) / (end_time - start_time):.2}")


def main():
    # model_path = "./base-models/qwen3-0.6b"
    model_path = "./base-models/Qwen3-4B-Instruct-2507"
    # model_path = "./base-models/Qwen3-4B-Thinking-2507"
    lora_config = LoraConfig()
    rngs = nnx.Rngs(0)
    model, tokenizer, sampling = load_model(model_path, lora_config, rngs)

    batch_size = 1
    seq_length = 16384  # 512
    chat(Console(), model, tokenizer, sampling, batch_size, seq_length, rngs)


if __name__ == "__main__":
    main()
