import time
from typing import Any, Literal, NamedTuple
import jax
from jax import numpy as jnp
import numpy as np
from flax import nnx

# from tokenizers import Tokenizer
from distrax import Categorical
from transformers import PreTrainedTokenizerFast

from rich.console import Console
from rich.markdown import Markdown

from llmrl.config import LoraConfig, SamplingConfig
from llmrl.model import Qwen3
from llmrl.checkpoint import load_model

PAD_ID = 151643
EOS_1 = 151645
EOS_2 = 151643
# END_TOKEN = 151644


def encode_input(
    tokenizer: PreTrainedTokenizerFast,
    conversations: list[list[dict]] | list[dict],
):
    tokens = tokenizer.apply_chat_template(
        conversations,
        add_generation_prompt=True,
        return_tensors="np",
        # enable_thinking=False
    )
    return tokens


def decode_responses(
    tokenizer: PreTrainedTokenizerFast,
    context: jax.Array,
    start: jax.Array,
    end: jax.Array,
) -> list[str]:
    output = []
    for b in range(
        context.shape[0]
    ):  # this should actually only decode sequences that are finished, some might still be in progress
        print(f"{start[b]}:{end[b]}")
        output.append(tokenizer.decode(np.asarray(context[b, start[b] : end[b]])))

    return output


def sample(config: SamplingConfig, logits, rng_key):
    V = logits.shape[-1]
    k = min(config.top_k, V)

    topk_logits, topk_idx = jax.lax.top_k(logits, k)  # (..., k)
    topk_logits = topk_logits / config.temperature

    topk_probs = jax.nn.softmax(topk_logits, axis=-1)  # (..., k)

    cumsum = jnp.cumsum(topk_probs, axis=-1) - topk_probs
    masked_topk_logits = jnp.where(cumsum < config.top_p, topk_logits, -jnp.inf)

    sample_in_topk = jax.random.categorical(rng_key, masked_topk_logits, axis=-1)
    sample_in_topk = jnp.expand_dims(sample_in_topk, axis=-1)
    sampled_ids = jnp.take_along_axis(topk_idx, sample_in_topk, axis=-1).squeeze(-1)

    return sampled_ids


class GenerationState(NamedTuple):
    kv_cache: Any

    # where the model is in the context in term of evaluation
    gen_positions: jax.Array
    # how long the context is
    context_positions: jax.Array
    # this indicates where the prompt turn ends and the conversation turn starts
    turn_start_positions: jax.Array
    # true for prompt tokens and false for tokens generated by the model
    prompt_mask: jax.Array
    # tokens for the context, this includes both prompts and generated tokens
    context: jax.Array

    # gen variables
    # next_tokens: jax.Array
    turn_finished: jax.Array
    rng_key: jax.Array

    # rollout
    log_probs: jax.Array
    values: jax.Array


def create_generation_state(
    kv_cache, batch_size: int, seq_len: int, rng_key: jax.Array
) -> GenerationState:
    return GenerationState(
        kv_cache=kv_cache,
        gen_positions=jnp.zeros(batch_size, jnp.int32),
        context_positions=jnp.zeros(batch_size, jnp.int32),
        turn_start_positions=jnp.zeros(batch_size, jnp.int32),
        prompt_mask=jnp.zeros((batch_size, seq_len), jnp.bool_),
        context=jnp.zeros((batch_size, seq_len), jnp.int32),
        # next_tokens=jnp.zeros(batch_size, jnp.int32),
        turn_finished=jnp.zeros(batch_size, jnp.bool_),
        log_probs=jnp.zeros((batch_size, seq_len), jnp.float32),
        values=jnp.zeros((batch_size, seq_len), jnp.float32),
        rng_key=rng_key,
    )


def reset_generation_state(state: GenerationState) -> GenerationState:
    return state._replace(
        gen_positions=jnp.zeros_like(state.gen_positions),
        context_positions=jnp.zeros_like(state.context_positions),
        turn_state_positions=jnp.zeros_like(state.turn_start_positions),
        prompt_mask=jnp.zeros_like(state.prompt_mask),
        finished=jnp.zeros_like(state.turn_finished),
    )


def add_prompts(
    state: GenerationState, batch_indices: list[int], prompts: list[np.ndarray]
) -> GenerationState:
    context = state.context
    prompt_mask = state.prompt_mask
    # todo: set the turn_start_positions and context size here

    for i, prompt in zip(batch_indices, prompts):
        start = state.context_positions[i]
        end = start + prompt.shape[0]
        context = context.at[i, start:end].set(prompt)
        prompt_mask = prompt_mask.at[i, start:end].set(np.ones_like(prompt, np.bool_))

    return state._replace(context=context, prompt_mask=prompt_mask)


@jax.jit(static_argnames=("model_def", "sampling"), donate_argnames=("gen",))
def generate(
    model_def,
    model_state,
    sampling: SamplingConfig | Literal["greedy", "simple"],
    gen: GenerationState,
) -> GenerationState:
    model = nnx.merge(model_def, model_state)

    B, seq_length = gen.context.shape
    batch_index = jnp.arange(B, dtype=jnp.int32)

    def cond(carry: GenerationState):
        # todo: let's check for n number of finished episodes
        return jnp.logical_not(jnp.any(carry.turn_finished))

    def body(carry: GenerationState):
        next_tokens = carry.context[batch_index, carry.gen_positions]

        logits, value, kv_cache = model(
            next_tokens[..., None],
            carry.gen_positions[..., None],
            carry.kv_cache,
        )

        sample_key, rng_key = jax.random.split(carry.rng_key)

        log_prob = None
        if sampling == "greedy":
            sample_tokens = jnp.argmax(logits.squeeze(-2), -1)
        elif sampling == "simple":
            dist = Categorical(logits=logits)
            sample_tokens: jax.Array = dist.sample(seed=sample_key)
            log_prob = dist.log_prob(sample_tokens)
        else:
            sample_tokens = sample(sampling, logits.squeeze(axis=-2), sample_key)

        gen_positions = jnp.where(
            carry.turn_finished, carry.gen_positions, carry.gen_positions + 1
        )

        use_sample = gen_positions > carry.turn_start_positions
        next_tokens = jnp.where(
            use_sample,
            sample_tokens,
            carry.context[batch_index, gen_positions]
        )

        next_context = carry.context.at[batch_index, gen_positions].set(
            next_tokens
        )

        # check for finished states
        next_finished = jnp.logical_or(carry.turn_finished, gen_positions >= seq_length - 1)
        # we check to make sure we aren't reading the prompt and that the model sampled a end token
        next_finished = jnp.logical_or(
            next_finished, jnp.logical_and(sample_tokens == EOS_1, use_sample)
        )

        return carry._replace(
            kv_cache=kv_cache,
            gen_positions=gen_positions,
            context_positions=jnp.maximum(carry.context_positions, gen_positions),
            context=next_context,
            finished=next_finished,
        )

    return nnx.while_loop(cond, body, gen)


def chat(
    console: Console,
    model: Qwen3,
    tokenizer,
    sampling,
    batch_size: int,
    seq_length: int,
    rngs: nnx.Rngs,
):
    model_def, model_state = nnx.split(model)

    rng_key = rngs.sample()
    kv_cache = model.initialize_carry(batch_size, seq_length)

    gen = create_generation_state(kv_cache, batch_size, seq_length, rng_key)

    while True:
        prompt = console.input("Prompt:\n")

        if prompt == "/clear":
            gen = reset_generation_state(gen)
            continue

        prompt_tokens, prompt_length = encode_input(
            tokenizer, conversations, seq_length
        )

        gen = gen._replace(context=prompt_tokens)
        rollout = None  # RolloutState()

        start_time = time.time()
        start_pos = gen.positions.copy()

        gen, rollout = generate(model_def, model_state, sampling, gen, rollout)

        output_text: list[str] = decode_responses(
            tokenizer, gen.context, prompt_length, gen.positions
        )
        for conv, out in zip(conversations, output_text):
            assistant = out  # get_last_turn(out)
            console.print("--------")
            console.print(Markdown(assistant))
            conv.append({"role": "assistant", "content": assistant})

        stop_time = time.time()
        delta_time = stop_time - start_time
        total_tokens = jnp.sum(gen.positions - start_pos).item()
        console.print(f"TPS: {total_tokens // delta_time}")
        console.print(f"Context: {gen.positions[0].item()}/{seq_length}")


def main():
    model_path = "./base-models/qwen3-0.6b"
    # model_path = "./base-models/Qwen3-4B-Instruct-2507"
    lora_config = LoraConfig(False, False, 0)
    rngs = nnx.Rngs(0)
    model, tokenizer, sampling = load_model(model_path, lora_config, rngs)

    batch_size = 1
    seq_length = 16384  # 512
    chat(Console(), model, tokenizer, sampling, batch_size, seq_length, rngs)


if __name__ == "__main__":
    main()
