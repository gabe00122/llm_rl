import time
from typing import Any, Literal, NamedTuple, cast
from jax import Array, numpy as jnp
import numpy as np
import jax
from flax import nnx
import optax
# from tokenizers import Tokenizer
from distrax import Categorical
from transformers import PreTrainedTokenizerFast

from rich.console import Console
from rich.markdown import Markdown

from llmrl.config import LoraConfig, SamplingConfig
from llmrl.model import Qwen3
from llmrl.checkpoint import load_model, load_safetensors

PAD_ID = 151643
EOS_1 = 151645
EOS_2 = 151643
# END_TOKEN = 151644


def encode_input(tokenizer: PreTrainedTokenizerFast, conversations: list[list[dict]] | list[dict], pad_size: int):
    inputs = tokenizer.apply_chat_template(
        conversations,
        padding='max_length',
        max_length=pad_size,
        add_generation_prompt=True,
        return_tensors='np'
    )
    return jnp.array(inputs)


def get_last_turn(text: str) -> str:
    llm_response = text.split("<|im_start|>assistant\n")[-1]
    return llm_response.split("<|im_end|>")[0]


def sample(config: SamplingConfig, logits, rng_key):
    V = logits.shape[-1]
    k = min(config.top_k, V)

    topk_logits, topk_idx = jax.lax.top_k(logits, k)  # (..., k)
    topk_logits = topk_logits / config.temperature

    topk_probs = jax.nn.softmax(topk_logits, axis=-1)  # (..., k)

    cumsum = jnp.cumsum(topk_probs, axis=-1) - topk_probs
    masked_topk_logits = jnp.where(cumsum < config.top_p, topk_logits, -jnp.inf)

    sample_in_topk = jax.random.categorical(rng_key, masked_topk_logits, axis=-1)
    sample_in_topk = jnp.expand_dims(sample_in_topk, axis=-1)
    sampled_ids = jnp.take_along_axis(topk_idx, sample_in_topk, axis=-1).squeeze(-1)

    return sampled_ids


class GenerationState(NamedTuple):
    # is_prompt: jax.Array # True if this was passed from the environment, False if it was generated by a agent
    kv_cache: Any
    positions: jax.Array
    context: jax.Array
    rng_key: jax.Array


class RolloutState(NamedTuple):
    log_probs: jax.Array
    values: jax.Array


@jax.jit(static_argnames=("model_def", "sampling"), donate_argnames=("gen", "rollout"))
def generate(
        model_def,
        model_state,
        sampling: SamplingConfig | Literal["greedy", "simple"],
        gen: GenerationState,
        rollout: RolloutState
    ) -> tuple[GenerationState, RolloutState]:
    model = nnx.merge(model_def, model_state)

    B, seq_length = gen.context.shape
    batch_index = jnp.arange(B, dtype=jnp.int32)

    class GenerateCarry(NamedTuple):
        next_tokens: jax.Array
        finished: jax.Array
        gen: GenerationState
        rollout: RolloutState
    
    def cond(carry: GenerateCarry):
        return jnp.logical_not(jnp.all(carry.finished))
    
    def body(carry: GenerateCarry):
        logits, value, kv_cache = model(carry.next_tokens[..., None], carry.gen.positions[..., None], carry.gen.kv_cache)

        sample_key, rng_key = jax.random.split(carry.gen.rng_key)

        log_prob = None
        if sampling == "greedy":
            sample_tokens = jnp.argmax(logits.squeeze(-2), -1)
        elif sampling == "simple":
            dist = Categorical(logits=logits)
            sample_tokens: jax.Array = dist.sample(seed=sample_key)
            log_prob = dist.log_prob(sample_tokens)
        else:
            sample_tokens = sample(sampling, logits.squeeze(axis=-2), sample_key)

        next_positions = jnp.where(carry.finished, carry.gen.positions, carry.gen.positions + 1)

        prompt_tokens = carry.gen.context[batch_index, next_positions]
        use_sample = jnp.logical_and(jnp.logical_or(prompt_tokens == PAD_ID, carry.finished), jnp.logical_not(carry.finished))
        next_tokens =  jnp.where(
            use_sample,
            sample_tokens,
            prompt_tokens
        )

        next_context = carry.gen.context.at[batch_index, next_positions].set(next_tokens)
        next_finished = jnp.logical_or(carry.finished, next_positions >= seq_length - 1)

        # we check to make sure we aren't reading the prompt and that the model sampled a end token
        next_finished = jnp.logical_or(next_finished, jnp.logical_and(sample_tokens == EOS_1, use_sample))

        return GenerateCarry(
            next_tokens,
            next_finished,
            gen=GenerationState(
                kv_cache,
                next_positions,
                next_context,
                rng_key,
            ),
            rollout=carry.rollout,
        )

    init_carry = GenerateCarry(
        gen.context[batch_index, gen.positions],
        jnp.zeros((B,), jnp.bool_),
        gen,
        rollout,
    )

    out = nnx.while_loop(cond, body, init_carry)

    return out.gen, out.rollout


def chat(console: Console, model: Qwen3, tokenizer, sampling, batch_size: int, seq_length: int, rngs: nnx.Rngs):
    model_def, model_state = nnx.split(model)
    
    rng_key = rngs.sample()
    kv_cache = model.initialize_carry(batch_size, seq_length)
    positions = jnp.zeros((batch_size,), jnp.int32)

    gen = GenerationState(
        kv_cache,
        positions,
        jnp.zeros((batch_size, seq_length), dtype=jnp.int32),
        rng_key,
    )

    conversations = []
    for _ in range(batch_size):
        conversations.append([])

    while True:
        prompt = console.input("Prompt: ")

        if prompt == "/clear":
            positions = jnp.zeros((batch_size,), jnp.int32)
            conversations = []
            for _ in range(batch_size):
                conversations.append([])
            continue

        for conv in conversations:
            conv.append({"role": "user", "content": prompt})
        prompt_tokens = encode_input(tokenizer, conversations, seq_length)
        
        gen = gen._replace(context=prompt_tokens)
        rollout = None #RolloutState()

        start_time = time.time()
        start_pos = gen.positions.copy()

        gen, rollout = generate(model_def, model_state, sampling, gen, rollout)

        output_text: list[str] = tokenizer.batch_decode(np.asarray(gen.context))
        for conv, out in zip(conversations, output_text):
            assistant = get_last_turn(out)
            console.print("--------")
            console.print(Markdown(assistant))
            conv.append({"role": "assistant", "content": assistant})
        
        stop_time = time.time()
        delta_time = stop_time - start_time
        total_tokens = jnp.sum(gen.positions - start_pos).item()
        console.print(f"TPS: {total_tokens // delta_time}")
        console.print(f"Context: {gen.positions[0].item()}/{seq_length}")



def main():
    # model_path = "./base-models/qwen3-0.6b"
    model_path = "./base-models/Qwen3-4B-Instruct-2507"
    lora_config = LoraConfig(False, False, 0)
    rngs = nnx.Rngs(0)
    model, tokenizer, sampling = load_model(model_path, lora_config, rngs)

    batch_size = 1
    seq_length = 16384 #512
    chat(Console(), model, tokenizer, sampling, batch_size, seq_length, rngs)


if __name__ == "__main__":
    main()
